{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/envs/py36/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "# import socket and json\n",
    "import socket\n",
    "from time import sleep\n",
    "import pickle\n",
    "\n",
    "# Import required modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetAllTextFromContentBody(contentBody):\n",
    "    out_str = ''\n",
    "    for x in contentBody:\n",
    "        out_str += x.text\n",
    "        \n",
    "    return out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetDateOfArticle(publishedData):\n",
    "    time_, GMT = publishedData.split('+')\n",
    "    dt = datetime.datetime.strptime(time_, '%Y-%m-%dT%H:%M:%S').strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetDateTimeOfArticle(soup):\n",
    "    metatags = soup.find_all('meta',attrs={'name':'publish-date'})\n",
    "    for tag in metatags:\n",
    "        return GetDateOfArticle(tag['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetSentimentValueFromNLTK(articleText):\n",
    "    \n",
    "    scores = dict([('pos', 0), ('neu', 0), ('neg', 0), ('compound', 0)])\n",
    "    \n",
    "    if not articleText:\n",
    "        return scores\n",
    "    \n",
    "    raw_text = articleText\n",
    "    raw_text = re.sub(\"\\n\", \". \", str(raw_text))\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentences = tokenize.sent_tokenize(raw_text)\n",
    "\n",
    "    scores = dict([('pos', 0), ('neu', 0), ('neg', 0), ('compound', 0)])\n",
    "    COUNT = 0\n",
    "    for sentence in sentences:\n",
    "        COUNT += 1\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "\n",
    "        for k in sorted(ss):            \n",
    "            scores[k] += ss[k]\n",
    "            \n",
    "    if COUNT != 0:        \n",
    "        scores['pos'] /= COUNT\n",
    "        scores['neu'] /= COUNT\n",
    "        scores['neg'] /= COUNT\n",
    "        scores['compound'] /= COUNT\n",
    "        \n",
    "        scores['pos'] = round(scores['pos'],2)\n",
    "        scores['neu'] = round(scores['neu'],2)\n",
    "        scores['neg'] = round(scores['neg'],2)\n",
    "        scores['compound'] = round(scores['compound'],2)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def CheckCompanyNameInArticle(articleText):\n",
    "    \n",
    "#    if 'companyName' in articleText.lower().split():\n",
    "#        return 1\n",
    "\n",
    "#    return 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetDataFromLinks(column):\n",
    "        r = requests.get(x)\n",
    "        html_content = r.text\n",
    "        soup = BeautifulSoup(html_content , 'lxml')\n",
    "        contentbody = soup.find_all(\"div\", id=lambda value: value and value.startswith(\"content-body\"))\n",
    "        articleText = GetAllTextFromContentBody(contentbody)\n",
    "                \n",
    "        scores = GetSentimentValueFromNLTK(articleText)\n",
    "        articleDate = GetDateTimeOfArticle(soup)\n",
    "        \n",
    "        seperator = ','\n",
    "                \n",
    "        #arr = ([str(articleDate.encode(\"utf-8\"))],[str(score.encode(\"utf-8\"))])\n",
    "        #arr = ([str(articleDate)],[str(score)])\n",
    "        #data_string = pickle.dumps(arr)                    \n",
    "        #data_string = str(articleDate).encode('utf-8')+ str(seperator).encode('utf-8') + str(score).encode('utf-8') +'\\n'\n",
    "        \n",
    "        data_string_Seq = (str(articleDate),str(scores['pos']),str(scores['neu']),str(scores['neg']),str(scores['compound']))\n",
    "        data_string = (seperator.join( data_string_Seq ))\n",
    "        return data_string + '\\n'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-02-20 17:27:35,0.06,0.76,0.04,0.03\n",
      "\n",
      "2016-02-20 15:33:11,0.03,0.69,0.03,0.05\n",
      "\n",
      "2016-02-20 14:00:12,0.06,0.72,0.07,-0.17\n",
      "\n",
      "2016-02-20 15:04:47,0.06,0.87,0.02,0.05\n",
      "\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('all_links.csv')\n",
    "columnData = df['links']\n",
    "\n",
    "with open('SentimentalScore.txt','a') as tf:\n",
    "    for x in columnData:            \n",
    "        dataFromLink = GetDataFromLinks(x)\n",
    "        print (dataFromLink)\n",
    "        tf.write(dataFromLink)\n",
    "        tf.write('\\n')\n",
    "        \n",
    "print(\"Completed\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
